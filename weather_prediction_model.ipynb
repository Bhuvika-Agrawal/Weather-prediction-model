{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgQLFZeakB2vsauFaGAbaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhuvika-Agrawal/Weather-prediction-model/blob/main/weather_prediction_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Introduction\n",
        "\n",
        "This project aims to predict maximum (tmax) and minimum (tmin) temperatures using historical weather data. We will preprocess the data, train a RandomForestRegressor, and evaluate its performance.\n"
      ],
      "metadata": {
        "id": "JBHMvRCzw3Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "P9o8PN0lxXpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading and Preprocessing\n",
        "\n",
        "We'll load the weather data CSV files, clean missing values, preprocess features, and prepare the dataset for model training.\n"
      ],
      "metadata": {
        "id": "UBjWQdPE5ulc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process uploaded files\n",
        "def process_files():\n",
        "    dataframes = []\n",
        "\n",
        "    # List files in the current directory\n",
        "    file_list = os.listdir()\n",
        "\n",
        "    for file_name in file_list:\n",
        "        if file_name.endswith(\".csv\"):\n",
        "            # Extract the city name from the file name\n",
        "            city_name = file_name.split('_')[0]\n",
        "            print(f\"Processing file: {file_name}, Extracted city name: {city_name}\")\n",
        "\n",
        "            # Read the CSV file into a DataFrame\n",
        "            try:\n",
        "                df = pd.read_csv(file_name, usecols=[\"time\", \"tavg\", \"tmax\", \"tmin\", \"prcp\"], dtype={\"time\": str})\n",
        "            except ValueError as e:\n",
        "                print(f\"Error reading {file_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Add the city name as a new column in the DataFrame\n",
        "            df['city'] = city_name\n",
        "\n",
        "            # Clean the 'time' column if needed\n",
        "            df['time'] = df['time'].str.strip()\n",
        "\n",
        "            print(f\"DataFrame for {city_name}:\")\n",
        "            print(df.head().to_string(index=False))  # Display without index\n",
        "\n",
        "            # Append the DataFrame to the list\n",
        "            dataframes.append(df)\n",
        "\n",
        "    # Check if any DataFrames were created\n",
        "    if not dataframes:\n",
        "        print(\"No DataFrames were created. Please check your CSV files.\")\n",
        "        return None\n",
        "\n",
        "    # Concatenate all DataFrames into a single DataFrame\n",
        "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "    # Drop duplicates if needed\n",
        "    combined_df.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Standardize column names (lowercase and strip whitespace)\n",
        "    combined_df.columns = [col.lower().strip() for col in combined_df.columns]\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "# Process the files\n",
        "combined_df = process_files()\n",
        "\n",
        "# Verify the combined DataFrame before saving\n",
        "if combined_df is not None:\n",
        "    print(\"Combined DataFrame before saving:\")\n",
        "    print(combined_df.head().to_string(index=False))  # Display without index\n",
        "\n",
        "    output_file = 'cleaned_weather_data.csv'\n",
        "    combined_df.to_csv(output_file, index=False)\n",
        "    print(f\"Combined DataFrame saved to: {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPqZrYUxxbEU",
        "outputId": "4e62d799-65c8-40d3-af5c-fcce46c83397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: Rajasthan_1990_2022_Jodhpur.csv, Extracted city name: Rajasthan\n",
            "DataFrame for Rajasthan:\n",
            "      time  tavg  tmin  tmax  prcp      city\n",
            "01-01-1990  22.9  19.1  28.4   NaN Rajasthan\n",
            "02-01-1990  21.7   NaN  26.5   0.0 Rajasthan\n",
            "03-01-1990  21.0  16.4  26.5   0.0 Rajasthan\n",
            "04-01-1990  20.8   NaN  27.4   0.0 Rajasthan\n",
            "05-01-1990  20.4  14.2  26.1   0.0 Rajasthan\n",
            "Processing file: Mumbai_1990_2022_Santacruz.csv, Extracted city name: Mumbai\n",
            "DataFrame for Mumbai:\n",
            "      time  tavg  tmin  tmax  prcp   city\n",
            "01-01-1990  23.2  17.0   NaN   0.0 Mumbai\n",
            "02-01-1990  22.2  16.5  29.9   0.0 Mumbai\n",
            "03-01-1990  21.8  16.3  30.7   0.0 Mumbai\n",
            "04-01-1990  25.4  17.9  31.8   0.0 Mumbai\n",
            "05-01-1990  26.5  19.3  33.7   0.0 Mumbai\n",
            "Processing file: Chennai_1990_2022_Madras.csv, Extracted city name: Chennai\n",
            "DataFrame for Chennai:\n",
            "      time  tavg  tmin  tmax  prcp    city\n",
            "01-01-1990  25.2  22.8  28.4   0.5 Chennai\n",
            "02-01-1990  24.9  21.7  29.1   0.0 Chennai\n",
            "03-01-1990  25.6  21.4  29.8   0.0 Chennai\n",
            "04-01-1990  25.7   NaN  28.7   0.0 Chennai\n",
            "05-01-1990  25.5  20.7  28.4   0.0 Chennai\n",
            "Processing file: Delhi_NCR_1990_2022_Safdarjung.csv, Extracted city name: Delhi\n",
            "DataFrame for Delhi:\n",
            "      time  tavg  tmin  tmax  prcp  city\n",
            "01-01-1990   9.4   6.0  15.1   0.0 Delhi\n",
            "02-01-1990   9.3   5.2  14.2   0.0 Delhi\n",
            "03-01-1990   9.0   6.5  13.6   0.0 Delhi\n",
            "04-01-1990  10.7   6.0  17.5   0.0 Delhi\n",
            "05-01-1990  12.6   7.3  20.8   0.0 Delhi\n",
            "Processing file: Lucknow_1990_2022.csv, Extracted city name: Lucknow\n",
            "DataFrame for Lucknow:\n",
            "      time  tavg  tmin  tmax  prcp    city\n",
            "01-01-1990   7.2   NaN  18.1   0.0 Lucknow\n",
            "02-01-1990  10.5   NaN  17.2   0.0 Lucknow\n",
            "03-01-1990  10.2   1.8  18.6   NaN Lucknow\n",
            "04-01-1990   9.1   NaN  19.3   0.0 Lucknow\n",
            "05-01-1990  13.5   NaN  23.8   0.0 Lucknow\n",
            "Processing file: Bangalore_1990_2022_BangaloreCity.csv, Extracted city name: Bangalore\n",
            "DataFrame for Bangalore:\n",
            "      time  tavg  tmin  tmax  prcp      city\n",
            "01-01-1990  22.9  19.1  28.4   NaN Bangalore\n",
            "02-01-1990  21.7   NaN  26.5   0.0 Bangalore\n",
            "03-01-1990  21.0  16.4  26.5   0.0 Bangalore\n",
            "04-01-1990  20.8   NaN  27.4   0.0 Bangalore\n",
            "05-01-1990  20.4  14.2  26.1   0.0 Bangalore\n",
            "Combined DataFrame before saving:\n",
            "      time  tavg  tmin  tmax  prcp      city\n",
            "01-01-1990  22.9  19.1  28.4   NaN Rajasthan\n",
            "02-01-1990  21.7   NaN  26.5   0.0 Rajasthan\n",
            "03-01-1990  21.0  16.4  26.5   0.0 Rajasthan\n",
            "04-01-1990  20.8   NaN  27.4   0.0 Rajasthan\n",
            "05-01-1990  20.4  14.2  26.1   0.0 Rajasthan\n",
            "Combined DataFrame saved to: cleaned_weather_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Missing Values\n",
        "\n",
        "We'll handle missing values using KNN imputation to ensure the dataset is complete and ready for analysis.\n"
      ],
      "metadata": {
        "id": "9LqICd3b6L3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate numeric and non-numeric columns\n",
        "numeric_features = combined_df.select_dtypes(include=['number']).columns\n",
        "non_numeric_features = combined_df.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Initialize the KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# Impute missing values on numeric features\n",
        "numeric_features_imputed = imputer.fit_transform(combined_df[numeric_features])\n",
        "\n",
        "# Create a DataFrame from the imputed numeric features and combine it with non-numeric features\n",
        "features_imputed = pd.DataFrame(numeric_features_imputed, columns=numeric_features)\n",
        "features = pd.concat([features_imputed, combined_df[non_numeric_features]], axis=1)\n",
        "\n",
        "print(\"Missing values handled.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvG_DSeNzfKq",
        "outputId": "06a21636-e8c4-4c4f-f485-1e9763dc2d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values handled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering\n",
        "\n",
        "We will create new features based on existing ones to improve the model's performance.\n"
      ],
      "metadata": {
        "id": "mx70slte83bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering\n",
        "combined_df['temp_diff'] = combined_df['tmax'] - combined_df['tmin']\n",
        "combined_df['avg_temp'] = (combined_df['tmax'] + combined_df['tmin']) / 2\n",
        "combined_df['time'] = pd.to_datetime(combined_df['time'], format='%d-%m-%Y')\n",
        "\n",
        "# Sorting by time\n",
        "combined_df.sort_values('time', inplace=True)\n",
        "\n",
        "# Adding lag features\n",
        "combined_df['prev_day_tmax'] = combined_df['tmax'].shift(1)\n",
        "combined_df['prev_day_prcp'] = combined_df['prcp'].shift(1)\n",
        "\n",
        "# Calculate rolling mean and standard deviation for the features\n",
        "combined_df['rolling_mean_7'] = combined_df['prcp'].rolling(window=7).mean()\n",
        "combined_df['rolling_std_7'] = combined_df['prcp'].rolling(window=7).std()\n",
        "combined_df['rolling_mean_30'] = combined_df['prcp'].rolling(window=30).mean()\n",
        "combined_df['rolling_std_30'] = combined_df['prcp'].rolling(window=30).std()\n",
        "\n",
        "# Fill NaN values resulting from rolling calculations\n",
        "combined_df.fillna(method='bfill', inplace=True)\n"
      ],
      "metadata": {
        "id": "oi_6GwPvzrkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling\n",
        "\n",
        "Next we will scale the features to bring them into a consistent range.\n"
      ],
      "metadata": {
        "id": "5r776FAA9MgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale features using Min-Max Scaler\n",
        "scaler = MinMaxScaler()\n",
        "combined_df[['tavg', 'tmax', 'tmin']] = scaler.fit_transform(combined_df[['tavg', 'tmax', 'tmin']])\n"
      ],
      "metadata": {
        "id": "17cDcuAf9Bqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test Split\n",
        "\n",
        "Splitting the dataset into training and testing sets to evaluate model performance.\n"
      ],
      "metadata": {
        "id": "_4sdAWPp9tPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target variables\n",
        "X = features.drop(columns=['time', 'city', 'tmax', 'tmin'])\n",
        "y_tmax = features['tmax']\n",
        "y_tmin = features['tmin']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train_tmax, y_test_tmax = train_test_split(X, y_tmax, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train_tmin, y_test_tmin = train_test_split(X, y_tmin, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "V6V9DLi69hsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "We'll train a Random Forest Regressor model to predict Tmax and Tmin.\n"
      ],
      "metadata": {
        "id": "I1t4gbwb-TN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Random Forest Regressor for Tmax and Tmin\n",
        "model_tmax = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model_tmin = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "# Fit the models\n",
        "model_tmax.fit(X_train, y_train_tmax)\n",
        "model_tmin.fit(X_train, y_train_tmin)\n",
        "# Set up cross-validation (e.g., 5-fold cross-validation)\n",
        "num_folds = 5\n",
        "\n",
        "# Perform cross-validation for Tmax\n",
        "scores_tmax = cross_validate(model_tmax, X_train, y_train_tmax, cv=num_folds, scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'], return_train_score=True)\n",
        "\n",
        "# Perform cross-validation for Tmin\n",
        "scores_tmin = cross_validate(model_tmin, X_train, y_train_tmin, cv=num_folds, scoring=['neg_mean_squared_error', 'neg_mean_absolute_error'], return_train_score=True)\n"
      ],
      "metadata": {
        "id": "GO4tigP0-Kov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Evaluating the trained model on the test set using Mean Squared Error (MSE) and Mean Absolute Error (MAE).\n"
      ],
      "metadata": {
        "id": "ya97pX5c-qNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate average scores for Tmax\n",
        "mse_tmax_cv = -1 * scores_tmax['test_neg_mean_squared_error'].mean()\n",
        "mae_tmax_cv = -1 * scores_tmax['test_neg_mean_absolute_error'].mean()\n",
        "\n",
        "# Calculate average scores for Tmin\n",
        "mse_tmin_cv = -1 * scores_tmin['test_neg_mean_squared_error'].mean()\n",
        "mae_tmin_cv = -1 * scores_tmin['test_neg_mean_absolute_error'].mean()\n",
        "\n",
        "print(f\"Tmax - Cross-validation MSE: {mse_tmax_cv:.4f}, MAE: {mae_tmax_cv:.4f}\")\n",
        "print(f\"Tmin - Cross-validation MSE: {mse_tmin_cv:.4f}, MAE: {mae_tmin_cv:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ncf3jo0c-cmX",
        "outputId": "af438fb3-824d-4d37-f77f-48fc635ddca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tmax - Cross-validation MSE: 2.9629, MAE: 1.3081\n",
            "Tmin - Cross-validation MSE: 3.5784, MAE: 1.4287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##prediction\n",
        "Using the trained models (`model_tmax` and `model_tmin`), we make predictions on the test set (`X_test`).\n",
        "Calculating R², Mean Squared error MSE and Mean absolute error MAE."
      ],
      "metadata": {
        "id": "_DmPP3_iXjCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions for Tmax and Tmin\n",
        "y_pred_tmax = model_tmax.predict(X_test)\n",
        "y_pred_tmin = model_tmin.predict(X_test)\n",
        "\n",
        "# Calculate R-squared\n",
        "r2_tmax = r2_score(y_test_tmax, y_pred_tmax)\n",
        "r2_tmin = r2_score(y_test_tmin, y_pred_tmin)\n",
        "\n",
        "# Calculate MSE and MAE\n",
        "mse_tmax = mean_squared_error(y_test_tmax, y_pred_tmax)\n",
        "mae_tmax = mean_absolute_error(y_test_tmax, y_pred_tmax)\n",
        "\n",
        "mse_tmin = mean_squared_error(y_test_tmin, y_pred_tmin)\n",
        "mae_tmin = mean_absolute_error(y_test_tmin, y_pred_tmin)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Tmax - R²: {r2_tmax:.4f}, MSE: {mse_tmax:.4f}, MAE: {mae_tmax:.4f}\")\n",
        "print(f\"Tmin - R²: {r2_tmin:.4f}, MSE: {mse_tmin:.4f}, MAE: {mae_tmin:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZnlaXxu7-x70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0050834-9089-4a40-e7c9-6980e62ee1f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tmax - R²: 0.8675, MSE: 2.9113, MAE: 1.2947\n",
            "Tmin - R²: 0.8764, MSE: 3.4317, MAE: 1.3985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJro6394U_ET"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}